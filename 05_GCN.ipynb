{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the following notebook we will continue our efforts to design gcn for leak detection and localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### highlighting the baseline problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20235678\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "C:\\Users\\20235678\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20235678\\AppData\\Local\\Temp\\ipykernel_18592\\2265896864.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load('graph_data_new_topology.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data loaded successfully!\n",
      "Graph structure: Data(edge_index=[2, 34], y_leak_detection=[17520000], y_location_1=[17520000], y_location_2=[17520000], node_features=[32, 17520000], num_nodes=32, edge_features=[34, 17520000])\n",
      "Processing graph data for leak detection task...\n",
      "Graph data prepared. Target variable: y_leak_detection\n",
      "Preparing time-series data chunks focused on leak events...\n",
      "Prepared 40955 chunks of data with leak events within a time window of 100.\n",
      "Splitting data into training and testing sets...\n",
      "Data split completed. Training and testing loaders created.\n",
      "Initializing GCN model and training components...\n",
      "Initializing GCN model...\n",
      "GCN model initialized successfully!\n",
      "Model initialized on device: cpu\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20235678\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.2232\n",
      "Epoch 2/50, Loss: 2.1877\n",
      "Epoch 3/50, Loss: 2.1878\n",
      "Epoch 4/50, Loss: 2.1877\n",
      "Epoch 5/50, Loss: 2.1879\n",
      "\n",
      "Starting testing...\n",
      "\n",
      "Results for threshold = 0.3\n",
      "Confusion Matrix:\n",
      "[[      0   44127]\n",
      " [      0 2003673]]\n",
      "Accuracy: 0.9785\n",
      "Precision: 0.9785\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9891\n",
      "\n",
      "Results for threshold = 0.4\n",
      "Confusion Matrix:\n",
      "[[      0   44127]\n",
      " [      0 2003673]]\n",
      "Accuracy: 0.9785\n",
      "Precision: 0.9785\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9891\n",
      "\n",
      "Results for threshold = 0.5\n",
      "Confusion Matrix:\n",
      "[[      0   44127]\n",
      " [      0 2003673]]\n",
      "Accuracy: 0.9785\n",
      "Precision: 0.9785\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9891\n",
      "\n",
      "Results for threshold = 0.6\n",
      "Confusion Matrix:\n",
      "[[      0   44127]\n",
      " [      0 2003673]]\n",
      "Accuracy: 0.9785\n",
      "Precision: 0.9785\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9891\n",
      "\n",
      "Results for threshold = 0.7\n",
      "Confusion Matrix:\n",
      "[[      0   44127]\n",
      " [      0 2003673]]\n",
      "Accuracy: 0.9785\n",
      "Precision: 0.9785\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9891\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Linear, Dropout, BatchNorm1d\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the graph data\n",
    "print(\"Loading graph data...\")\n",
    "graph_data = torch.load('graph_data_new_topology.pt')\n",
    "print(\"Graph data loaded successfully!\")\n",
    "print(\"Graph structure:\", graph_data)\n",
    "\n",
    "# Step 2: Use only the required features\n",
    "print(\"Processing graph data for leak detection task...\")\n",
    "graph_data.y = graph_data.y_leak_detection  # Set the leak detection target\n",
    "del graph_data.y_location_1, graph_data.y_location_2  # Remove unused labels\n",
    "print(\"Graph data prepared. Target variable: y_leak_detection\")\n",
    "\n",
    "# Step 3: Define the GCN Model with More Layers and Dropout\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_prob=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        print(\"Initializing GCN model...\")\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        print(\"GCN model initialized successfully!\")\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add batch normalization and residual connections\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Residual connection for conv2\n",
    "        x_res = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x + x_res)  # Add residual connection\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Residual connection for conv3\n",
    "        x_res = x\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x + x_res)  # Add residual connection\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.sigmoid(x)  # Explicitly apply sigmoid for probabilities\n",
    "        return x\n",
    "\n",
    "# Step 4: Prepare data chunks for leak event-based training\n",
    "print(\"Preparing time-series data chunks focused on leak events...\")\n",
    "time_window = 100\n",
    "num_nodes = graph_data.num_nodes\n",
    "num_steps = graph_data.node_features.size(1)\n",
    "chunks = []\n",
    "\n",
    "# Iterate over the time steps, focus on chunks where leak events occur\n",
    "for start in range(0, num_steps - time_window + 1, time_window):\n",
    "    end = start + time_window\n",
    "    x_chunk = graph_data.node_features[:, start:end].T\n",
    "    y_chunk = graph_data.y[start:end]\n",
    "    \n",
    "    # Filter only chunks with leak events (y_chunk contains leak detection labels)\n",
    "    if torch.any(y_chunk == 1):  # Look for leak events (assume 1 indicates a leak)\n",
    "        edge_index = graph_data.edge_index\n",
    "        chunks.append(Data(x=x_chunk, edge_index=edge_index, y=y_chunk))\n",
    "\n",
    "print(f\"Prepared {len(chunks)} chunks of data with leak events within a time window of {time_window}.\")\n",
    "\n",
    "# Step 5: Split data into training and testing sets\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "train_chunks, test_chunks = train_test_split(chunks, test_size=0.5, random_state=42)\n",
    "train_loader = DataLoader(train_chunks, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_chunks, batch_size=32, shuffle=False)\n",
    "print(\"Data split completed. Training and testing loaders created.\")\n",
    "\n",
    "# Step 6: Initialize model, optimizer, and loss function with early stopping\n",
    "print(\"Initializing GCN model and training components...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "in_channels = num_nodes\n",
    "hidden_channels = 32  # Increased hidden channels\n",
    "out_channels = 1\n",
    "\n",
    "model = GCN(in_channels, hidden_channels, out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Reduced learning rate\n",
    "criterion = torch.nn.BCELoss()  # Use BCELoss instead of BCEWithLogitsLoss\n",
    "print(f\"Model initialized on device: {device}\")\n",
    "\n",
    "# Step 7: Training loop with early stopping\n",
    "def train_model(patience=5):\n",
    "    print(\"Starting training...\")\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(5):  # Increased epochs\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x.float(), batch.edge_index).view(-1)  # Model already outputs probabilities\n",
    "            loss = criterion(out, batch.y.float())  # BCELoss works with probabilities\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/50, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                model.load_state_dict(torch.load('best_model.pt'))\n",
    "                break\n",
    "\n",
    "def test_model():\n",
    "    print(\"\\nStarting testing...\")\n",
    "    model.eval()\n",
    "    all_true_labels = []\n",
    "    all_predictions = []  # Store probabilities directly\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x.float(), batch.edge_index).view(-1)  # Output is already probabilities\n",
    "            all_true_labels.extend(batch.y.cpu().numpy())\n",
    "            all_predictions.extend(out.cpu().numpy())  # No need for sigmoid here\n",
    "    \n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nResults for threshold = {threshold}\")\n",
    "        pred_labels = (all_predictions > threshold).astype(int)\n",
    "        \n",
    "        cm = confusion_matrix(all_true_labels, pred_labels)\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        \n",
    "        metrics = {\n",
    "            'Accuracy': accuracy_score(all_true_labels, pred_labels),\n",
    "            'Precision': precision_score(all_true_labels, pred_labels, zero_division=1),  # Handle zero division\n",
    "            'Recall': recall_score(all_true_labels, pred_labels, zero_division=1),  # Handle zero division\n",
    "            'F1 Score': f1_score(all_true_labels, pred_labels, zero_division=1)  # Handle zero division\n",
    "        }\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starting fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/gs1k_4jd57q1qn2qphzx45l40000gn/T/ipykernel_22792/3465531707.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load('graph_data_new_topology.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data loaded successfully!\n",
      "Graph structure: Data(edge_index=[2, 34], y_leak_detection=[17520000], y_location_1=[17520000], y_location_2=[17520000], node_features=[32, 17520000], num_nodes=32, edge_features=[34, 17520000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Linear, Dropout, BatchNorm1d\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# Step 1: Load the graph data\n",
    "print(\"Loading graph data...\")\n",
    "graph_data = torch.load('graph_data_new_topology.pt')\n",
    "print(\"Graph data loaded successfully!\")\n",
    "print(\"Graph structure:\", graph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing with smaller dataset of 1 million data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated graph data structure:\n",
      "y_leak_detection: torch.Size([1000000])\n",
      "y_location_1: torch.Size([1000000])\n",
      "y_location_2: torch.Size([1000000])\n",
      "node_features: torch.Size([32, 1000000])\n",
      "edge_features: torch.Size([34, 1000000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Clip the graph data to the first million data points\n",
    "num_data_points = 1_000_000  # Define the limit for the data points\n",
    "\n",
    "# Clipping relevant fields\n",
    "graph_data.y_leak_detection = graph_data.y_leak_detection[:num_data_points]\n",
    "graph_data.y_location_1 = graph_data.y_location_1[:num_data_points]\n",
    "graph_data.y_location_2 = graph_data.y_location_2[:num_data_points]\n",
    "graph_data.node_features = graph_data.node_features[:, :num_data_points]\n",
    "graph_data.edge_features = graph_data.edge_features[:, :num_data_points]\n",
    "\n",
    "# Verify the updated data shape\n",
    "print(\"Updated graph data structure:\")\n",
    "print(\"y_leak_detection:\", graph_data.y_leak_detection.shape)\n",
    "print(\"y_location_1:\", graph_data.y_location_1.shape)\n",
    "print(\"y_location_2:\", graph_data.y_location_2.shape)\n",
    "print(\"node_features:\", graph_data.node_features.shape)\n",
    "print(\"edge_features:\", graph_data.edge_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph structure: Data(edge_index=[2, 34], y_leak_detection=[1000000], y_location_1=[1000000], y_location_2=[1000000], node_features=[32, 1000000], num_nodes=32, edge_features=[34, 1000000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Graph structure:\", graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph data for leak detection task...\n",
      "Graph data prepared. Target variable: y_leak_detection\n",
      "Using device: cpu\n",
      "Down-sampling node and edge features...\n",
      "Down-sampling complete.\n",
      "Balancing dataset with SMOTE...\n",
      "Reshaped y_flat shape: (99999,)\n",
      "Number of samples in x_flat: 99999\n",
      "Number of samples in y_flat_resized: 99999\n",
      "SMOTE completed. Dataset balanced.\n",
      "Instantiating the model...\n",
      "Initializing GCN model...\n",
      "GCN model initialized successfully!\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x180780 and 32x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Train for 10 epochs (can adjust as needed)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(output\u001b[38;5;241m.\u001b[39msqueeze(), graph_data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m    113\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Add batch normalization and residual connections\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/GNN/.venv/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x180780 and 32x64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "print(\"Processing graph data for leak detection task...\")\n",
    "# Assume `graph_data` is already loaded from the .pt file\n",
    "graph_data.y = graph_data.y_leak_detection  # Set the target for leak detection\n",
    "del graph_data.y_location_1, graph_data.y_location_2  # Remove unused labels\n",
    "print(\"Graph data prepared. Target variable: y_leak_detection\")\n",
    "\n",
    "# Step 2: Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 3: Down-sample the data with a sliding window approach\n",
    "def downsample_with_sliding_window(features, window_size=20, overlap=10):\n",
    "    stride = window_size - overlap\n",
    "    num_windows = (features.shape[1] - window_size) // stride + 1\n",
    "    downsampled_features = []\n",
    "    for i in range(num_windows):\n",
    "        start = i * stride\n",
    "        end = start + window_size\n",
    "        window_avg = features[:, start:end].mean(axis=1)\n",
    "        downsampled_features.append(window_avg)\n",
    "    return np.stack(downsampled_features, axis=1)\n",
    "\n",
    "print(\"Down-sampling node and edge features...\")\n",
    "node_features = graph_data.node_features.cpu().numpy()\n",
    "edge_features = graph_data.edge_features.cpu().numpy()\n",
    "\n",
    "downsampled_node_features = downsample_with_sliding_window(node_features, window_size=20, overlap=10)\n",
    "downsampled_edge_features = downsample_with_sliding_window(edge_features, window_size=20, overlap=10)\n",
    "\n",
    "graph_data.node_features = torch.tensor(downsampled_node_features, dtype=torch.float).to(device)\n",
    "graph_data.edge_features = torch.tensor(downsampled_edge_features, dtype=torch.float).to(device)\n",
    "print(\"Down-sampling complete.\")\n",
    "\n",
    "# Step 4: Balance the dataset with SMOTE\n",
    "print(\"Balancing dataset with SMOTE...\")\n",
    "x_flat = graph_data.node_features.T.cpu().numpy()  # Flatten features for SMOTE\n",
    "y_flat = graph_data.y.cpu().numpy()\n",
    "\n",
    "# Reshaping y_flat to match the number of samples in x_flat\n",
    "desired_num_labels = x_flat.shape[0]  # Ensure y_flat has the same number of samples as x_flat\n",
    "y_flat_resized = y_flat[:desired_num_labels]  # Adjust size (or repeat if necessary)\n",
    "\n",
    "print(f\"Reshaped y_flat shape: {y_flat_resized.shape}\")\n",
    "print(f\"Number of samples in x_flat: {x_flat.shape[0]}\")\n",
    "print(f\"Number of samples in y_flat_resized: {y_flat_resized.shape[0]}\")\n",
    "\n",
    "smote = SMOTE()\n",
    "x_resampled, y_resampled = smote.fit_resample(x_flat, y_flat_resized)\n",
    "\n",
    "# Restore node features back to graph format\n",
    "graph_data.node_features = torch.tensor(x_resampled.T, dtype=torch.float).to(device)\n",
    "graph_data.y = torch.tensor(y_resampled, dtype=torch.float).to(device)\n",
    "print(\"SMOTE completed. Dataset balanced.\")\n",
    "\n",
    "# Step 5: Define the GCN model with residual connections and dropout\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_prob=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        print(\"Initializing GCN model...\")\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        print(\"GCN model initialized successfully!\")\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add batch normalization and residual connections\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Residual connection for conv2\n",
    "        x_res = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x + x_res)  # Add residual connection\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Residual connection for conv3\n",
    "        x_res = x\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x + x_res)  # Add residual connection\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.sigmoid(x)  # Explicitly apply sigmoid for probabilities\n",
    "        return x\n",
    "\n",
    "# Step 6: Instantiate and train the model\n",
    "print(\"Instantiating the model...\")\n",
    "in_channels = graph_data.node_features.shape[0]\n",
    "hidden_channels = 64\n",
    "out_channels = 1\n",
    "\n",
    "model = GCN(in_channels, hidden_channels, out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(10):  # Train for 10 epochs (can adjust as needed)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(graph_data.node_features, graph_data.edge_index)\n",
    "    loss = F.binary_cross_entropy(output.squeeze(), graph_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Step 7: Test the model\n",
    "print(\"Testing the model...\")\n",
    "train_size = int(0.8 * graph_data.node_features.shape[1])  # 80% train, 20% test\n",
    "train_features = graph_data.node_features[:, :train_size].to(device)\n",
    "train_labels = graph_data.y[:train_size].to(device)\n",
    "test_features = graph_data.node_features[:, train_size:].to(device)\n",
    "test_labels = graph_data.y[train_size:].to(device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_features, graph_data.edge_index.to(device))\n",
    "\n",
    "# Convert predictions to binary class labels (0 or 1)\n",
    "predicted_labels = (predictions.squeeze() > 0.5).float()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(test_labels.cpu().numpy(), predicted_labels.cpu().numpy())\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels.cpu().numpy(), predicted_labels.cpu().numpy())\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels.cpu().numpy(), predicted_labels.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# claude proposed fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph data for leak detection task...\n",
      "\n",
      "Initial data shapes:\n",
      "Node features: torch.Size([32, 1000000])\n",
      "Edge index: torch.Size([2, 34])\n",
      "Target variable shape: torch.Size([1000000])\n",
      "\n",
      "Reshaping data for GCN...\n",
      "Reshaped node features: torch.Size([32, 99999])\n",
      "Node labels shape: torch.Size([32])\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "Starting training...\n",
      "Epoch 10/100, Loss: 1.0411\n",
      "Epoch 20/100, Loss: 0.7278\n",
      "Epoch 30/100, Loss: 0.7279\n",
      "Epoch 40/100, Loss: 0.6703\n",
      "Epoch 50/100, Loss: 0.7818\n",
      "Epoch 60/100, Loss: 0.6919\n",
      "Epoch 70/100, Loss: 0.6386\n",
      "Epoch 80/100, Loss: 0.7120\n",
      "Epoch 90/100, Loss: 0.6778\n",
      "Epoch 100/100, Loss: 0.6807\n",
      "\n",
      "Evaluating model...\n",
      "Accuracy: 0.5312\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.07      0.12        15\n",
      "         1.0       0.53      0.94      0.68        17\n",
      "\n",
      "    accuracy                           0.53        32\n",
      "   macro avg       0.52      0.50      0.40        32\n",
      "weighted avg       0.52      0.53      0.42        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data preparation with correct shapes\n",
    "print(\"Processing graph data for leak detection task...\")\n",
    "print(\"\\nInitial data shapes:\")\n",
    "print(f\"Node features: {graph_data.node_features.shape}\")\n",
    "print(f\"Edge index: {graph_data.edge_index.shape}\")\n",
    "print(f\"Target variable shape: {graph_data.y_leak_detection.shape}\")\n",
    "\n",
    "# The key insight is that for GCN, we need:\n",
    "# - Node features shape: [num_nodes, num_features]\n",
    "# - Each node should have a feature vector\n",
    "# - The time series should be treated as features, not nodes\n",
    "\n",
    "def reshape_for_gcn(node_features, window_size=20, stride=10):\n",
    "    \"\"\"\n",
    "    Reshape time series data into appropriate GCN format\n",
    "    Returns: Features with shape [num_nodes, num_features]\n",
    "    \"\"\"\n",
    "    # First, reshape to handle the time series properly\n",
    "    num_nodes = node_features.shape[0]  # 32 nodes\n",
    "    num_timesteps = node_features.shape[1]  # 1000000 timesteps\n",
    "    \n",
    "    # Calculate number of windows\n",
    "    num_windows = (num_timesteps - window_size) // stride + 1\n",
    "    \n",
    "    # Create feature matrix where each window becomes a feature\n",
    "    features = torch.zeros((num_nodes, num_windows))\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        # Average over the window to create a feature\n",
    "        features[:, i] = node_features[:, start_idx:end_idx].mean(dim=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Reshape data\n",
    "print(\"\\nReshaping data for GCN...\")\n",
    "node_features = reshape_for_gcn(graph_data.node_features, window_size=20, stride=10)\n",
    "print(f\"Reshaped node features: {node_features.shape}\")\n",
    "\n",
    "# Create target variable for nodes\n",
    "# We'll aggregate the time series labels to get one label per node\n",
    "def aggregate_labels(y_leak_detection, num_nodes):\n",
    "    \"\"\"\n",
    "    Aggregate time series labels to get one label per node\n",
    "    Uses majority voting to determine node label\n",
    "    \"\"\"\n",
    "    y_reshaped = y_leak_detection.view(num_nodes, -1)\n",
    "    # A node is considered affected if it has leaks in more than 25% of timesteps\n",
    "    threshold = 0.25\n",
    "    node_labels = (y_reshaped.float().mean(dim=1) > threshold).float()\n",
    "    return node_labels\n",
    "\n",
    "y_node = aggregate_labels(graph_data.y_leak_detection, node_features.shape[0])\n",
    "print(f\"Node labels shape: {y_node.shape}\")\n",
    "\n",
    "# Step 2: Define the GCN model with proper dimensions\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, dropout_prob=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        # in_channels is now the number of features per node\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, 1)  # Output one value per node\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Step 3: Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Move data to device\n",
    "node_features = node_features.to(device)\n",
    "edge_index = graph_data.edge_index.to(device)\n",
    "y_node = y_node.to(device)\n",
    "\n",
    "# Initialize model\n",
    "in_channels = node_features.shape[1]  # Number of features per node\n",
    "model = GCN(in_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Step 4: Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "model.train()\n",
    "num_epochs = 100  # Increased epochs since we have less data now\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(node_features, edge_index)\n",
    "    loss = F.binary_cross_entropy(out.squeeze(), y_node)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(node_features, edge_index)\n",
    "    pred_labels = (pred.squeeze() > 0.5).float()\n",
    "    accuracy = accuracy_score(y_node.cpu(), pred_labels.cpu())\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_node.cpu(), pred_labels.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a working model, lets improve it with chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/gs1k_4jd57q1qn2qphzx45l40000gn/T/ipykernel_22920/3465531707.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load('graph_data_new_topology.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data loaded successfully!\n",
      "Graph structure: Data(edge_index=[2, 34], y_leak_detection=[17520000], y_location_1=[17520000], y_location_2=[17520000], node_features=[32, 17520000], num_nodes=32, edge_features=[34, 17520000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Linear, Dropout, BatchNorm1d\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# Step 1: Load the graph data\n",
    "print(\"Loading graph data...\")\n",
    "graph_data = torch.load('graph_data_new_topology.pt')\n",
    "print(\"Graph data loaded successfully!\")\n",
    "print(\"Graph structure:\", graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated graph data structure:\n",
      "y_leak_detection: torch.Size([1000000])\n",
      "y_location_1: torch.Size([1000000])\n",
      "y_location_2: torch.Size([1000000])\n",
      "node_features: torch.Size([32, 1000000])\n",
      "edge_features: torch.Size([34, 1000000])\n",
      "Graph structure: Data(edge_index=[2, 34], y_leak_detection=[1000000], y_location_1=[1000000], y_location_2=[1000000], node_features=[32, 1000000], num_nodes=32, edge_features=[34, 1000000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Clip the graph data to the first million data points\n",
    "num_data_points = 1_000_000  # Define the limit for the data points\n",
    "\n",
    "# Clipping relevant fields\n",
    "graph_data.y_leak_detection = graph_data.y_leak_detection[:num_data_points]\n",
    "graph_data.y_location_1 = graph_data.y_location_1[:num_data_points]\n",
    "graph_data.y_location_2 = graph_data.y_location_2[:num_data_points]\n",
    "graph_data.node_features = graph_data.node_features[:, :num_data_points]\n",
    "graph_data.edge_features = graph_data.edge_features[:, :num_data_points]\n",
    "\n",
    "# Verify the updated data shape\n",
    "print(\"Updated graph data structure:\")\n",
    "print(\"y_leak_detection:\", graph_data.y_leak_detection.shape)\n",
    "print(\"y_location_1:\", graph_data.y_location_1.shape)\n",
    "print(\"y_location_2:\", graph_data.y_location_2.shape)\n",
    "print(\"node_features:\", graph_data.node_features.shape)\n",
    "print(\"edge_features:\", graph_data.edge_features.shape)\n",
    "print(\"Graph structure:\", graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph data for leak detection task...\n",
      "\n",
      "Initial data shapes:\n",
      "Node features: torch.Size([32, 1000000])\n",
      "Edge features: torch.Size([34, 1000000])\n",
      "Edge index: torch.Size([2, 34])\n",
      "Target variable shape: torch.Size([1000000])\n",
      "\n",
      "Reshaping data for GCN...\n",
      "Reshaped node features: torch.Size([32, 99999])\n",
      "Reshaped edge features: torch.Size([34, 99999])\n",
      "Node labels shape: torch.Size([32])\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "Starting training...\n",
      "Epoch 10/1000, Loss: 40.6562\n",
      "Epoch 20/1000, Loss: 40.6415\n",
      "Epoch 30/1000, Loss: 40.6369\n",
      "Epoch 40/1000, Loss: 40.6373\n",
      "Epoch 50/1000, Loss: 40.6394\n",
      "Epoch 60/1000, Loss: 40.6407\n",
      "Epoch 70/1000, Loss: 40.6403\n",
      "Epoch 80/1000, Loss: 40.6397\n",
      "Epoch 90/1000, Loss: 40.6396\n",
      "Epoch 100/1000, Loss: 40.6398\n",
      "Epoch 110/1000, Loss: 40.6399\n",
      "Epoch 120/1000, Loss: 40.6398\n",
      "Epoch 130/1000, Loss: 40.6398\n",
      "Epoch 140/1000, Loss: 40.6398\n",
      "Epoch 150/1000, Loss: 40.6398\n",
      "Epoch 160/1000, Loss: 40.6398\n",
      "Epoch 170/1000, Loss: 40.6398\n",
      "Epoch 180/1000, Loss: 40.6398\n",
      "Epoch 190/1000, Loss: 40.6398\n",
      "Epoch 200/1000, Loss: 40.6398\n",
      "Epoch 210/1000, Loss: 40.6398\n",
      "Epoch 220/1000, Loss: 40.6398\n",
      "Epoch 230/1000, Loss: 40.6398\n",
      "Epoch 240/1000, Loss: 40.6398\n",
      "Epoch 250/1000, Loss: 40.6398\n",
      "Epoch 260/1000, Loss: 40.6398\n",
      "Epoch 270/1000, Loss: 40.6398\n",
      "Epoch 280/1000, Loss: 40.6398\n",
      "Epoch 290/1000, Loss: 40.6398\n",
      "Epoch 300/1000, Loss: 40.6398\n",
      "Epoch 310/1000, Loss: 40.6398\n",
      "Epoch 320/1000, Loss: 40.6398\n",
      "Epoch 330/1000, Loss: 40.6398\n",
      "Epoch 340/1000, Loss: 40.6398\n",
      "Epoch 350/1000, Loss: 40.6398\n",
      "Epoch 360/1000, Loss: 40.6398\n",
      "Epoch 370/1000, Loss: 40.6398\n",
      "Epoch 380/1000, Loss: 40.6398\n",
      "Epoch 390/1000, Loss: 40.6398\n",
      "Epoch 400/1000, Loss: 40.6398\n",
      "Epoch 410/1000, Loss: 40.6398\n",
      "Epoch 420/1000, Loss: 40.6398\n",
      "Epoch 430/1000, Loss: 40.6398\n",
      "Epoch 440/1000, Loss: 40.6398\n",
      "Epoch 450/1000, Loss: 40.6398\n",
      "Epoch 460/1000, Loss: 40.6398\n",
      "Epoch 470/1000, Loss: 40.6398\n",
      "Epoch 480/1000, Loss: 40.6398\n",
      "Epoch 490/1000, Loss: 40.6398\n",
      "Epoch 500/1000, Loss: 40.6398\n",
      "Epoch 510/1000, Loss: 40.6398\n",
      "Epoch 520/1000, Loss: 40.6398\n",
      "Epoch 530/1000, Loss: 40.6398\n",
      "Epoch 540/1000, Loss: 40.6398\n",
      "Epoch 550/1000, Loss: 40.6398\n",
      "Epoch 560/1000, Loss: 40.6398\n",
      "Epoch 570/1000, Loss: 40.6398\n",
      "Epoch 580/1000, Loss: 40.6398\n",
      "Epoch 590/1000, Loss: 40.6398\n",
      "Epoch 600/1000, Loss: 40.6412\n",
      "Epoch 610/1000, Loss: 40.6388\n",
      "Epoch 620/1000, Loss: 40.6402\n",
      "Epoch 630/1000, Loss: 40.6398\n",
      "Epoch 640/1000, Loss: 40.6397\n",
      "Epoch 650/1000, Loss: 40.6399\n",
      "Epoch 660/1000, Loss: 40.6397\n",
      "Epoch 670/1000, Loss: 40.6398\n",
      "Epoch 680/1000, Loss: 40.6401\n",
      "Epoch 690/1000, Loss: 40.6396\n",
      "Epoch 700/1000, Loss: 40.6399\n",
      "Epoch 710/1000, Loss: 40.6398\n",
      "Epoch 720/1000, Loss: 40.6398\n",
      "Epoch 730/1000, Loss: 40.6398\n",
      "Epoch 740/1000, Loss: 40.6398\n",
      "Epoch 750/1000, Loss: 40.6398\n",
      "Epoch 760/1000, Loss: 40.6398\n",
      "Epoch 770/1000, Loss: 40.6398\n",
      "Epoch 780/1000, Loss: 40.6398\n",
      "Epoch 790/1000, Loss: 40.6398\n",
      "Epoch 800/1000, Loss: 40.6398\n",
      "Epoch 810/1000, Loss: 40.6398\n",
      "Epoch 820/1000, Loss: 40.6398\n",
      "Epoch 830/1000, Loss: 40.6398\n",
      "Epoch 840/1000, Loss: 40.6398\n",
      "Epoch 850/1000, Loss: 40.6398\n",
      "Epoch 860/1000, Loss: 40.6398\n",
      "Epoch 870/1000, Loss: 40.6368\n",
      "Epoch 880/1000, Loss: 50.0171\n",
      "Epoch 890/1000, Loss: 65.6388\n",
      "Epoch 900/1000, Loss: 65.6401\n",
      "Epoch 910/1000, Loss: 62.5148\n",
      "Epoch 920/1000, Loss: 46.8897\n",
      "Epoch 930/1000, Loss: 56.2649\n",
      "Epoch 940/1000, Loss: 40.6423\n",
      "Epoch 950/1000, Loss: 40.6380\n",
      "Epoch 960/1000, Loss: 40.6409\n",
      "Epoch 970/1000, Loss: 34.3895\n",
      "Epoch 980/1000, Loss: 37.5148\n",
      "Epoch 990/1000, Loss: 43.7649\n",
      "Epoch 1000/1000, Loss: 50.0147\n",
      "\n",
      "Evaluating model...\n",
      "Accuracy: 0.5938\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.13      0.24        15\n",
      "         1.0       0.57      1.00      0.72        17\n",
      "\n",
      "    accuracy                           0.59        32\n",
      "   macro avg       0.78      0.57      0.48        32\n",
      "weighted avg       0.77      0.59      0.49        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data preparation with correct shapes\n",
    "print(\"Processing graph data for leak detection task...\")\n",
    "\n",
    "# Print initial shapes\n",
    "print(\"\\nInitial data shapes:\")\n",
    "print(f\"Node features: {graph_data.node_features.shape}\")\n",
    "print(f\"Edge features: {graph_data.edge_features.shape}\")\n",
    "print(f\"Edge index: {graph_data.edge_index.shape}\")\n",
    "print(f\"Target variable shape: {graph_data.y_leak_detection.shape}\")\n",
    "\n",
    "# Reshape node features for GCN\n",
    "def reshape_for_gcn(node_features, window_size=20, stride=10):\n",
    "    \"\"\"\n",
    "    Reshape node time series data into appropriate GCN format.\n",
    "    Returns: Features with shape [num_nodes, num_features].\n",
    "    \"\"\"\n",
    "    num_nodes = node_features.shape[0]\n",
    "    num_timesteps = node_features.shape[1]\n",
    "    num_windows = (num_timesteps - window_size) // stride + 1\n",
    "    \n",
    "    features = torch.zeros((num_nodes, num_windows))\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        features[:, i] = node_features[:, start_idx:end_idx].mean(dim=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Reshape edge features for GCN\n",
    "def reshape_edge_features(edge_features, window_size=20, stride=10):\n",
    "    \"\"\"\n",
    "    Reshape edge time series data into appropriate GCN format.\n",
    "    Returns: Features with shape [num_edges, num_features].\n",
    "    \"\"\"\n",
    "    num_edges = edge_features.shape[0]\n",
    "    num_timesteps = edge_features.shape[1]\n",
    "    num_windows = (num_timesteps - window_size) // stride + 1\n",
    "    \n",
    "    features = torch.zeros((num_edges, num_windows))\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        features[:, i] = edge_features[:, start_idx:end_idx].mean(dim=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Reshape node and edge features\n",
    "print(\"\\nReshaping data for GCN...\")\n",
    "node_features = reshape_for_gcn(graph_data.node_features, window_size=20, stride=10)\n",
    "edge_features = reshape_edge_features(graph_data.edge_features, window_size=20, stride=10)\n",
    "print(f\"Reshaped node features: {node_features.shape}\")\n",
    "print(f\"Reshaped edge features: {edge_features.shape}\")\n",
    "\n",
    "# Aggregate time-series labels for nodes\n",
    "def aggregate_labels(y_leak_detection, num_nodes):\n",
    "    \"\"\"\n",
    "    Aggregate time series labels to get one label per node.\n",
    "    Uses majority voting to determine node label.\n",
    "    \"\"\"\n",
    "    y_reshaped = y_leak_detection.view(num_nodes, -1)\n",
    "    threshold = 0.25\n",
    "    node_labels = (y_reshaped.float().mean(dim=1) > threshold).float()\n",
    "    return node_labels\n",
    "\n",
    "y_node = aggregate_labels(graph_data.y_leak_detection, node_features.shape[0])\n",
    "print(f\"Node labels shape: {y_node.shape}\")\n",
    "\n",
    "# Step 2: Define the GCN model with edge features\n",
    "class GCNWithEdgeFeatures(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_channels, hidden_channels=64, dropout_prob=0.5):\n",
    "        super(GCNWithEdgeFeatures, self).__init__(aggr='add')  # Aggregation method (sum)\n",
    "        self.node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(edge_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.final_mlp = torch.nn.Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes, in_channels]\n",
    "        edge_index: Edge indices [2, num_edges]\n",
    "        edge_attr: Edge features [num_edges, edge_channels]\n",
    "        \"\"\"\n",
    "        x = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        return torch.sigmoid(self.final_mlp(x))\n",
    "    \n",
    "    def message(self, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        x_j: Features of source nodes [num_edges, hidden_channels]\n",
    "        edge_attr: Edge features [num_edges, edge_channels]\n",
    "        \"\"\"\n",
    "        edge_messages = self.edge_mlp(edge_attr)\n",
    "        node_messages = self.node_mlp(x_j)\n",
    "        return edge_messages + node_messages\n",
    "\n",
    "# Step 3: Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Move data to device\n",
    "node_features = node_features.to(device)\n",
    "edge_index = graph_data.edge_index.to(device)\n",
    "edge_features = edge_features.to(device)\n",
    "y_node = y_node.to(device)\n",
    "\n",
    "# Initialize model\n",
    "in_channels = node_features.shape[1]  # Number of features per node\n",
    "edge_channels = edge_features.shape[1]  # Number of features per edge\n",
    "model = GCNWithEdgeFeatures(in_channels, edge_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.01)\n",
    "\n",
    "# Step 4: Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "model.train()\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(node_features, edge_index, edge_features)\n",
    "    loss = F.binary_cross_entropy(out.squeeze(), y_node)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(node_features, edge_index, edge_features)\n",
    "    pred_labels = (pred.squeeze() > 0.5).float()\n",
    "    accuracy = accuracy_score(y_node.cpu(), pred_labels.cpu())\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_node.cpu(), pred_labels.cpu(), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got good code we are almost there now we just have to use claude to add the deep learning techniuqes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n",
      "\n",
      "Starting training...\n",
      "Epoch 10/1000, Loss: 0.6578\n",
      "Epoch 20/1000, Loss: 0.6581\n",
      "Epoch 30/1000, Loss: 0.6442\n",
      "Epoch 40/1000, Loss: 0.6245\n",
      "Epoch 50/1000, Loss: 0.6506\n",
      "Epoch 60/1000, Loss: 0.6331\n",
      "Epoch 70/1000, Loss: 0.6260\n",
      "Epoch 80/1000, Loss: 0.6529\n",
      "Epoch 90/1000, Loss: 0.6989\n",
      "Epoch 100/1000, Loss: 0.6570\n",
      "Epoch 110/1000, Loss: 0.7361\n",
      "Epoch 120/1000, Loss: 0.7242\n",
      "Epoch 130/1000, Loss: 0.6800\n",
      "Epoch 140/1000, Loss: 0.6696\n",
      "Epoch 150/1000, Loss: 0.6349\n",
      "Epoch 160/1000, Loss: 0.6712\n",
      "Epoch 170/1000, Loss: 0.6841\n",
      "Epoch 180/1000, Loss: 0.6536\n",
      "Epoch 190/1000, Loss: 0.6698\n",
      "Epoch 200/1000, Loss: 0.6683\n",
      "Epoch 210/1000, Loss: 0.6883\n",
      "Epoch 220/1000, Loss: 0.6783\n",
      "Epoch 230/1000, Loss: 0.7249\n",
      "Epoch 240/1000, Loss: 1.1553\n",
      "Epoch 250/1000, Loss: 0.7067\n",
      "Epoch 260/1000, Loss: 0.6221\n",
      "Epoch 270/1000, Loss: 0.7414\n",
      "Epoch 280/1000, Loss: 0.6811\n",
      "Epoch 290/1000, Loss: 0.6840\n",
      "Epoch 300/1000, Loss: 0.6505\n",
      "Epoch 310/1000, Loss: 0.6704\n",
      "Epoch 320/1000, Loss: 0.7273\n",
      "Epoch 330/1000, Loss: 0.6438\n",
      "Epoch 340/1000, Loss: 0.6770\n",
      "Epoch 350/1000, Loss: 0.6770\n",
      "Epoch 360/1000, Loss: 0.6483\n",
      "Epoch 370/1000, Loss: 0.6821\n",
      "Epoch 380/1000, Loss: 0.6703\n",
      "Epoch 390/1000, Loss: 0.6996\n",
      "Epoch 400/1000, Loss: 0.6911\n",
      "Epoch 410/1000, Loss: 0.6970\n",
      "Epoch 420/1000, Loss: 0.6763\n",
      "Epoch 430/1000, Loss: 0.7511\n",
      "Epoch 440/1000, Loss: 0.6574\n",
      "Epoch 450/1000, Loss: 0.6622\n",
      "Epoch 460/1000, Loss: 0.6471\n",
      "Epoch 470/1000, Loss: 0.6881\n",
      "Epoch 480/1000, Loss: 0.6887\n",
      "Epoch 490/1000, Loss: 0.6886\n",
      "Epoch 500/1000, Loss: 0.6790\n",
      "Epoch 510/1000, Loss: 0.6669\n",
      "Epoch 520/1000, Loss: 0.6778\n",
      "Epoch 530/1000, Loss: 0.7031\n",
      "Epoch 540/1000, Loss: 0.6670\n",
      "Epoch 550/1000, Loss: 0.7171\n",
      "Epoch 560/1000, Loss: 0.6612\n",
      "Epoch 570/1000, Loss: 0.7849\n",
      "Epoch 580/1000, Loss: 0.6938\n",
      "Epoch 590/1000, Loss: 0.6842\n",
      "Epoch 600/1000, Loss: 0.6949\n",
      "Epoch 610/1000, Loss: 0.7276\n",
      "Epoch 620/1000, Loss: 0.7012\n",
      "Epoch 630/1000, Loss: 0.6783\n",
      "Epoch 640/1000, Loss: 0.6928\n",
      "Epoch 650/1000, Loss: 0.6645\n",
      "Epoch 660/1000, Loss: 0.6883\n",
      "Epoch 670/1000, Loss: 0.6895\n",
      "Epoch 680/1000, Loss: 0.6940\n",
      "Epoch 690/1000, Loss: 0.6832\n",
      "Epoch 700/1000, Loss: 0.6880\n",
      "Epoch 710/1000, Loss: 0.6387\n",
      "Epoch 720/1000, Loss: 0.9381\n",
      "Epoch 730/1000, Loss: 0.8255\n",
      "Epoch 740/1000, Loss: 0.8896\n",
      "Epoch 750/1000, Loss: 0.6967\n",
      "Epoch 760/1000, Loss: 0.7037\n",
      "Epoch 770/1000, Loss: 0.6829\n",
      "Epoch 780/1000, Loss: 0.6467\n",
      "Epoch 790/1000, Loss: 0.7244\n",
      "Epoch 800/1000, Loss: 0.6816\n",
      "Epoch 810/1000, Loss: 0.6872\n",
      "Epoch 820/1000, Loss: 0.6963\n",
      "Epoch 830/1000, Loss: 0.7042\n",
      "Epoch 840/1000, Loss: 0.6988\n",
      "Epoch 850/1000, Loss: 0.6739\n",
      "Epoch 860/1000, Loss: 0.6908\n",
      "Epoch 870/1000, Loss: 0.6816\n",
      "Epoch 880/1000, Loss: 0.7967\n",
      "Epoch 890/1000, Loss: 0.6833\n",
      "Epoch 900/1000, Loss: 0.7375\n",
      "Epoch 910/1000, Loss: 0.6820\n",
      "Epoch 920/1000, Loss: 0.9340\n",
      "Epoch 930/1000, Loss: 0.8948\n",
      "Epoch 940/1000, Loss: 0.6636\n",
      "Epoch 950/1000, Loss: 0.6880\n",
      "Epoch 960/1000, Loss: 0.6514\n",
      "Epoch 970/1000, Loss: 0.6778\n",
      "Epoch 980/1000, Loss: 0.6911\n",
      "Epoch 990/1000, Loss: 1.2798\n",
      "Epoch 1000/1000, Loss: 0.5475\n",
      "\n",
      "Evaluating model...\n",
      "Accuracy: 0.5938\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.20      0.32        15\n",
      "         1.0       0.57      0.94      0.71        17\n",
      "\n",
      "    accuracy                           0.59        32\n",
      "   macro avg       0.66      0.57      0.51        32\n",
      "weighted avg       0.66      0.59      0.53        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Enhanced GCN Model with improved structure and handling\n",
    "class EnhancedGCNWithEdgeFeatures(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_channels, hidden_channels=128, num_layers=3, dropout_prob=0.5):\n",
    "        \"\"\"\n",
    "        Enhanced GCN architecture with fixed dimensions for time series data.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input features per node (e.g., reshaped time series windows)\n",
    "            edge_channels: Number of features per edge (e.g., reshaped time series windows)\n",
    "            hidden_channels: Size of hidden layers\n",
    "            num_layers: Number of message passing layers\n",
    "            dropout_prob: Dropout probability\n",
    "        \"\"\"\n",
    "        super(EnhancedGCNWithEdgeFeatures, self).__init__(aggr='add')  # Aggregation by addition\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initial feature projection to hidden dimensions\n",
    "        self.node_projection = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.edge_projection = torch.nn.Linear(edge_channels, hidden_channels)\n",
    "\n",
    "        # Node processing layers\n",
    "        self.node_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.LayerNorm(hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Edge processing layers\n",
    "        self.edge_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.LayerNorm(hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Attention mechanism for node pairs\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "        # Skip connection transforms\n",
    "        self.skip_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "            for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        # Final prediction layers\n",
    "        self.final_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            torch.nn.LayerNorm(hidden_channels // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_prob),\n",
    "            torch.nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Forward pass with proper dimension handling.\n",
    "        \"\"\"\n",
    "        # Initial projection to hidden dimension\n",
    "        x = self.node_projection(x)  # [num_nodes, hidden_channels]\n",
    "        edge_attr = self.edge_projection(edge_attr)  # [num_edges, hidden_channels]\n",
    "\n",
    "        # Initialize previous layer output\n",
    "        previous_layer = None\n",
    "\n",
    "        # Process through multiple layers\n",
    "        for i in range(self.num_layers):\n",
    "            # Transform node features\n",
    "            current_x = self.node_transforms[i](x if i == 0 else previous_layer)\n",
    "\n",
    "            # Add skip connection if not first layer\n",
    "            if i > 0:\n",
    "                current_x = current_x + self.skip_transforms[i - 1](previous_layer)\n",
    "\n",
    "            # Message passing with transformed edge features\n",
    "            current_x = self.propagate(\n",
    "                edge_index,\n",
    "                x=current_x,\n",
    "                edge_attr=self.edge_transforms[i](edge_attr)\n",
    "            )\n",
    "\n",
    "            # Store current layer output\n",
    "            previous_layer = current_x\n",
    "\n",
    "        # Final prediction\n",
    "        return torch.sigmoid(self.final_layers(current_x))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        Message function with attention for properly shaped tensors.\n",
    "        \"\"\"\n",
    "        # Compute attention weights\n",
    "        attention_input = torch.cat([x_i, x_j], dim=-1)  # [num_edges, hidden_channels * 2]\n",
    "        attention_weights = torch.softmax(self.attention(attention_input), dim=-1)\n",
    "\n",
    "        # Combine node and edge features\n",
    "        combined_features = x_j + edge_attr  # [num_edges, hidden_channels]\n",
    "\n",
    "        return attention_weights * combined_features\n",
    "\n",
    "\n",
    "# Modified training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Assuming node_features, edge_index, edge_features, and y_node are already prepared\n",
    "node_features = node_features.to(device)\n",
    "edge_index = graph_data.edge_index.to(device)\n",
    "edge_features = edge_features.to(device)\n",
    "y_node = y_node.to(device)\n",
    "\n",
    "# Initialize enhanced model\n",
    "in_channels = node_features.shape[1]\n",
    "edge_channels = edge_features.shape[1]\n",
    "model = EnhancedGCNWithEdgeFeatures(\n",
    "    in_channels=in_channels,\n",
    "    edge_channels=edge_channels,\n",
    "    hidden_channels=128,\n",
    "    num_layers=3,\n",
    "    dropout_prob=0.5\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.01)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "model.train()\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(node_features, edge_index, edge_features)\n",
    "    loss = F.binary_cross_entropy(out.squeeze(), y_node)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(node_features, edge_index, edge_features)\n",
    "    pred_labels = (pred.squeeze() > 0.5).float()\n",
    "    accuracy = accuracy_score(y_node.cpu(), pred_labels.cpu())\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_node.cpu(), pred_labels.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more advanced optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qusai/Desktop/GNN/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Train Loss: 0.6026, Val Loss: 0.5185\n",
      "Epoch 20/1000, Train Loss: 0.4443, Val Loss: 0.4999\n",
      "Epoch 30/1000, Train Loss: 0.5010, Val Loss: 0.4794\n",
      "Epoch 40/1000, Train Loss: 0.4827, Val Loss: 0.4789\n",
      "Epoch 50/1000, Train Loss: 0.5706, Val Loss: 0.4571\n",
      "Epoch 60/1000, Train Loss: 0.4620, Val Loss: 0.4260\n",
      "Epoch 70/1000, Train Loss: 0.4308, Val Loss: 0.4351\n",
      "Epoch 80/1000, Train Loss: 0.4321, Val Loss: 0.4286\n",
      "\n",
      "Early stopping triggered at epoch 84\n",
      "\n",
      "Evaluating model...\n",
      "Accuracy: 0.7500\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.93      0.78        15\n",
      "         1.0       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.75        32\n",
      "   macro avg       0.79      0.76      0.75        32\n",
      "weighted avg       0.80      0.75      0.74        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/gs1k_4jd57q1qn2qphzx45l40000gn/T/ipykernel_22920/1788968713.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Enhanced GCN Model with improved structure and handling\n",
    "class EnhancedGCNWithEdgeFeatures(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_channels, hidden_channels=128, num_layers=3, dropout_prob=0.5):\n",
    "        super(EnhancedGCNWithEdgeFeatures, self).__init__(aggr='add')\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.node_projection = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.edge_projection = torch.nn.Linear(edge_channels, hidden_channels)\n",
    "\n",
    "        self.node_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.LayerNorm(hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.edge_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.LayerNorm(hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "        self.skip_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "            for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        self.final_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            torch.nn.LayerNorm(hidden_channels // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_prob),\n",
    "            torch.nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.node_projection(x)\n",
    "        edge_attr = self.edge_projection(edge_attr)\n",
    "        previous_layer = None\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            current_x = self.node_transforms[i](x if i == 0 else previous_layer)\n",
    "            if i > 0:\n",
    "                current_x = current_x + self.skip_transforms[i - 1](previous_layer)\n",
    "            current_x = self.propagate(\n",
    "                edge_index,\n",
    "                x=current_x,\n",
    "                edge_attr=self.edge_transforms[i](edge_attr)\n",
    "            )\n",
    "            previous_layer = current_x\n",
    "\n",
    "        return torch.sigmoid(self.final_layers(current_x))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        attention_input = torch.cat([x_i, x_j], dim=-1)\n",
    "        attention_weights = torch.softmax(self.attention(attention_input), dim=-1)\n",
    "        combined_features = x_j + edge_attr\n",
    "        return attention_weights * combined_features\n",
    "\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Assuming node_features, edge_index, edge_features, and y_node are already prepared\n",
    "node_features = node_features.to(device)\n",
    "edge_index = graph_data.edge_index.to(device)\n",
    "edge_features = edge_features.to(device)\n",
    "y_node = y_node.to(device)\n",
    "\n",
    "# Initialize model\n",
    "in_channels = node_features.shape[1]\n",
    "edge_channels = edge_features.shape[1]\n",
    "model = EnhancedGCNWithEdgeFeatures(\n",
    "    in_channels=in_channels,\n",
    "    edge_channels=edge_channels,\n",
    "    hidden_channels=128,\n",
    "    num_layers=3,\n",
    "    dropout_prob=0.5\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 20\n",
    "best_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(node_features, edge_index, edge_features)\n",
    "    train_loss = F.binary_cross_entropy(out.squeeze(), y_node)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation (simulated here; replace with actual validation data)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(node_features, edge_index, edge_features)  # Replace with validation data\n",
    "        val_loss = F.binary_cross_entropy(val_out.squeeze(), y_node)  # Replace y_node with validation labels\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Scheduler and early stopping\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(node_features, edge_index, edge_features)\n",
    "    pred_labels = (pred.squeeze() > 0.5).float()\n",
    "    accuracy = accuracy_score(y_node.cpu(), pred_labels.cpu())\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_node.cpu(), pred_labels.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perfect now lets perform hyperparamaeter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3625620/1872215942.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load('graph_data_new_topology.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load and prepare the graph data\n",
    "print(\"Loading graph data...\")\n",
    "graph_data = torch.load('graph_data_new_topology.pt')\n",
    "print(\"Graph data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated graph data structure:\n",
      "y_leak_detection: torch.Size([1000000])\n",
      "y_location_1: torch.Size([1000000])\n",
      "y_location_2: torch.Size([1000000])\n",
      "node_features: torch.Size([32, 1000000])\n",
      "edge_features: torch.Size([34, 1000000])\n",
      "\n",
      "Reshaping data for GCN...\n",
      "Reshaped node features: torch.Size([32, 99999])\n",
      "Reshaped edge features: torch.Size([34, 99999])\n",
      "Node labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clip the graph data to the first million data points\n",
    "num_data_points = 1_000_000  # Define the limit for the data points\n",
    "graph_data.y_leak_detection = graph_data.y_leak_detection[:num_data_points]\n",
    "graph_data.y_location_1 = graph_data.y_location_1[:num_data_points]\n",
    "graph_data.y_location_2 = graph_data.y_location_2[:num_data_points]\n",
    "graph_data.node_features = graph_data.node_features[:, :num_data_points]\n",
    "graph_data.edge_features = graph_data.edge_features[:, :num_data_points]\n",
    "\n",
    "# Verify the updated data shape\n",
    "print(\"Updated graph data structure:\")\n",
    "print(f\"y_leak_detection: {graph_data.y_leak_detection.shape}\")\n",
    "print(f\"y_location_1: {graph_data.y_location_1.shape}\")\n",
    "print(f\"y_location_2: {graph_data.y_location_2.shape}\")\n",
    "print(f\"node_features: {graph_data.node_features.shape}\")\n",
    "print(f\"edge_features: {graph_data.edge_features.shape}\")\n",
    "\n",
    "# Step 3: Reshape node and edge features for GCN\n",
    "def reshape_for_gcn(data, window_size=20, stride=10):\n",
    "    \"\"\"\n",
    "    Reshape time-series data for GCN format. \n",
    "    Returns reshaped data with shape [num_elements, num_windows].\n",
    "    \"\"\"\n",
    "    num_elements, num_timesteps = data.shape\n",
    "    num_windows = (num_timesteps - window_size) // stride + 1\n",
    "    \n",
    "    reshaped_data = torch.zeros((num_elements, num_windows))\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        reshaped_data[:, i] = data[:, start_idx:end_idx].mean(dim=1)\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "# Reshape node and edge features\n",
    "print(\"\\nReshaping data for GCN...\")\n",
    "node_features = reshape_for_gcn(graph_data.node_features, window_size=20, stride=10)\n",
    "edge_features = reshape_for_gcn(graph_data.edge_features, window_size=20, stride=10)\n",
    "print(f\"Reshaped node features: {node_features.shape}\")\n",
    "print(f\"Reshaped edge features: {edge_features.shape}\")\n",
    "\n",
    "# Step 4: Aggregate labels for nodes using majority voting\n",
    "def aggregate_labels(y_leak_detection, num_nodes, threshold=0.25):\n",
    "    \"\"\"\n",
    "    Aggregate time-series labels for each node using majority voting.\n",
    "    \"\"\"\n",
    "    y_reshaped = y_leak_detection.view(num_nodes, -1)\n",
    "    node_labels = (y_reshaped.float().mean(dim=1) > threshold).float()\n",
    "    return node_labels\n",
    "\n",
    "y_node = aggregate_labels(graph_data.y_leak_detection, node_features.shape[0])\n",
    "print(f\"Node labels shape: {y_node.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 31250])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data.y_leak_detection.view(node_features.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31250.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000000 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-01-15 16:33:08,986] A new study created in memory with name: no-name-e7849f93-e607-4ea6-b32c-cb9cfd24b468\n",
      "/tmp/ipykernel_3625620/3807466596.py:86: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "/tmp/ipykernel_3625620/3807466596.py:87: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2025-01-15 16:33:10,749] Trial 0 finished with value: 0.42835503816604614 and parameters: {'hidden_channels': 128, 'num_layers': 2, 'dropout_prob': 0.2340553769186014, 'learning_rate': 0.0003653196180549422, 'weight_decay': 1.6177545066464346e-05}. Best is trial 0 with value: 0.42835503816604614.\n",
      "[I 2025-01-15 16:33:11,707] Trial 1 finished with value: 0.32135555148124695 and parameters: {'hidden_channels': 64, 'num_layers': 4, 'dropout_prob': 0.26259500016281323, 'learning_rate': 0.0007833955064294884, 'weight_decay': 0.003855047514345294}. Best is trial 1 with value: 0.32135555148124695.\n",
      "[I 2025-01-15 16:33:12,887] Trial 2 finished with value: 0.4708130955696106 and parameters: {'hidden_channels': 192, 'num_layers': 3, 'dropout_prob': 0.3510274381452122, 'learning_rate': 1.3437554034674018e-05, 'weight_decay': 6.691516311445385e-05}. Best is trial 1 with value: 0.32135555148124695.\n",
      "[I 2025-01-15 16:33:14,230] Trial 3 finished with value: 0.3552675247192383 and parameters: {'hidden_channels': 192, 'num_layers': 4, 'dropout_prob': 0.4857359457246318, 'learning_rate': 0.0001980402469973865, 'weight_decay': 9.892567941773585e-05}. Best is trial 1 with value: 0.32135555148124695.\n",
      "[I 2025-01-15 16:33:15,593] Trial 4 finished with value: 0.3136586844921112 and parameters: {'hidden_channels': 256, 'num_layers': 3, 'dropout_prob': 0.2267529776938736, 'learning_rate': 0.00014032816014298877, 'weight_decay': 5.0299909636026314e-05}. Best is trial 4 with value: 0.3136586844921112.\n",
      "[I 2025-01-15 16:33:16,427] Trial 5 finished with value: 0.4161469340324402 and parameters: {'hidden_channels': 128, 'num_layers': 2, 'dropout_prob': 0.3106519688052327, 'learning_rate': 0.0037712921447502676, 'weight_decay': 0.001194821818262672}. Best is trial 4 with value: 0.3136586844921112.\n",
      "[I 2025-01-15 16:33:17,259] Trial 6 finished with value: 0.4522811770439148 and parameters: {'hidden_channels': 128, 'num_layers': 2, 'dropout_prob': 0.383106627598965, 'learning_rate': 0.0005174182849176964, 'weight_decay': 0.00014890110673888455}. Best is trial 4 with value: 0.3136586844921112.\n",
      "[I 2025-01-15 16:33:18,333] Trial 7 finished with value: 0.19262294471263885 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.23491625492288323, 'learning_rate': 0.004541957045308247, 'weight_decay': 0.00156621507104819}. Best is trial 7 with value: 0.19262294471263885.\n",
      "[I 2025-01-15 16:33:19,226] Trial 8 finished with value: 0.5637322068214417 and parameters: {'hidden_channels': 192, 'num_layers': 2, 'dropout_prob': 0.44189713129062647, 'learning_rate': 3.522712631899136e-05, 'weight_decay': 0.0038822727169809746}. Best is trial 7 with value: 0.19262294471263885.\n",
      "[I 2025-01-15 16:33:19,828] Trial 9 finished with value: 0.4521671235561371 and parameters: {'hidden_channels': 64, 'num_layers': 3, 'dropout_prob': 0.4334254333644497, 'learning_rate': 0.0008401555766294122, 'weight_decay': 0.0006823363561268568}. Best is trial 7 with value: 0.19262294471263885.\n",
      "[I 2025-01-15 16:33:20,427] Trial 10 finished with value: 0.28050339221954346 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.2984559343862505, 'learning_rate': 0.009968835708294991, 'weight_decay': 0.009750589434293433}. Best is trial 7 with value: 0.19262294471263885.\n",
      "[I 2025-01-15 16:33:21,312] Trial 11 finished with value: 0.30300676822662354 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.29015639946094324, 'learning_rate': 0.009379542498069653, 'weight_decay': 0.00863306138240149}. Best is trial 7 with value: 0.19262294471263885.\n",
      "[I 2025-01-15 16:33:22,224] Trial 12 finished with value: 0.1878442019224167 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.2034137569034957, 'learning_rate': 0.0033736618359195138, 'weight_decay': 0.0014884414284854616}. Best is trial 12 with value: 0.1878442019224167.\n",
      "[I 2025-01-15 16:33:23,353] Trial 13 finished with value: 0.18370887637138367 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.2021856910813518, 'learning_rate': 0.002616020493103217, 'weight_decay': 0.0007013230361734503}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:24,518] Trial 14 finished with value: 0.24706779420375824 and parameters: {'hidden_channels': 128, 'num_layers': 4, 'dropout_prob': 0.20106137361834261, 'learning_rate': 0.0019770567886061587, 'weight_decay': 0.0003138253148328433}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:26,220] Trial 15 finished with value: 0.18849779665470123 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.20092580054624004, 'learning_rate': 0.0019401710262037487, 'weight_decay': 0.0004224008448602157}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:27,181] Trial 16 finished with value: 0.3024769723415375 and parameters: {'hidden_channels': 64, 'num_layers': 4, 'dropout_prob': 0.26953887969253687, 'learning_rate': 0.0016255532832801462, 'weight_decay': 0.0018728213681050761}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:28,271] Trial 17 finished with value: 0.20365571975708008 and parameters: {'hidden_channels': 128, 'num_layers': 5, 'dropout_prob': 0.34151067172349303, 'learning_rate': 0.0039173406413299475, 'weight_decay': 0.00023261365581149225}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:29,413] Trial 18 finished with value: 0.40246647596359253 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.25898477487485405, 'learning_rate': 8.507714997632423e-05, 'weight_decay': 0.0008367585188973362}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:30,565] Trial 19 finished with value: 0.267394483089447 and parameters: {'hidden_channels': 128, 'num_layers': 4, 'dropout_prob': 0.34364354099808314, 'learning_rate': 0.001053759119276141, 'weight_decay': 0.0030211156662309896}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:31,571] Trial 20 finished with value: 0.22557340562343597 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.3820372279692926, 'learning_rate': 0.005316341529902841, 'weight_decay': 0.0005454480390100329}. Best is trial 13 with value: 0.18370887637138367.\n",
      "[I 2025-01-15 16:33:33,247] Trial 21 finished with value: 0.18040581047534943 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.21068677915524262, 'learning_rate': 0.0017960995434738696, 'weight_decay': 0.00045305768381683896}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:34,882] Trial 22 finished with value: 0.18792477250099182 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.21816129904182863, 'learning_rate': 0.00240549768488466, 'weight_decay': 0.0009667720498837395}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:36,225] Trial 23 finished with value: 0.24045506119728088 and parameters: {'hidden_channels': 192, 'num_layers': 4, 'dropout_prob': 0.24937728906884526, 'learning_rate': 0.0012083917137896134, 'weight_decay': 0.00019898495703473334}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:37,524] Trial 24 finished with value: 0.24798111617565155 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.20055312775917813, 'learning_rate': 0.002695810133036851, 'weight_decay': 0.0020106598327206534}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:38,300] Trial 25 finished with value: 0.41095679998397827 and parameters: {'hidden_channels': 192, 'num_layers': 5, 'dropout_prob': 0.22540094078406553, 'learning_rate': 0.00618874919505504, 'weight_decay': 0.0004158517541896447}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:39,419] Trial 26 finished with value: 0.2698424458503723 and parameters: {'hidden_channels': 128, 'num_layers': 4, 'dropout_prob': 0.2758762056345952, 'learning_rate': 0.0005813018529149128, 'weight_decay': 0.0006228060640718145}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:41,030] Trial 27 finished with value: 0.1929612159729004 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.31640813009984425, 'learning_rate': 0.0013694364765769256, 'weight_decay': 0.005159327479784013}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:41,997] Trial 28 finished with value: 0.24948367476463318 and parameters: {'hidden_channels': 64, 'num_layers': 4, 'dropout_prob': 0.24691271403693535, 'learning_rate': 0.0029798008416382932, 'weight_decay': 3.675224072041391e-05}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:43,301] Trial 29 finished with value: 0.2470969557762146 and parameters: {'hidden_channels': 128, 'num_layers': 5, 'dropout_prob': 0.22034591963631184, 'learning_rate': 0.0002115249394328828, 'weight_decay': 1.1320069916908175e-05}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:44,504] Trial 30 finished with value: 0.3359539210796356 and parameters: {'hidden_channels': 192, 'num_layers': 3, 'dropout_prob': 0.2410834259837642, 'learning_rate': 0.0004972849284879214, 'weight_decay': 0.0012282906603738306}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:45,898] Trial 31 finished with value: 0.211049422621727 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.20824285116083655, 'learning_rate': 0.0023606847915432355, 'weight_decay': 0.0009552307653445679}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:47,142] Trial 32 finished with value: 0.2839818000793457 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.21999306600375063, 'learning_rate': 0.0066956862297432125, 'weight_decay': 0.002456599843894449}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:48,336] Trial 33 finished with value: 0.24650388956069946 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.23781151053159155, 'learning_rate': 0.0031785698262658656, 'weight_decay': 0.0002995250167455046}. Best is trial 21 with value: 0.18040581047534943.\n",
      "[I 2025-01-15 16:33:50,003] Trial 34 finished with value: 0.17314834892749786 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.21973929213009785, 'learning_rate': 0.00036458127448707456, 'weight_decay': 0.0001310522791222265}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:51,373] Trial 35 finished with value: 0.26584333181381226 and parameters: {'hidden_channels': 192, 'num_layers': 4, 'dropout_prob': 0.2817900321224193, 'learning_rate': 0.00036479072825276835, 'weight_decay': 0.00011873685873210785}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:53,050] Trial 36 finished with value: 0.21649070084095 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.21810112767746195, 'learning_rate': 7.646173880831005e-05, 'weight_decay': 2.854184831583674e-05}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:54,388] Trial 37 finished with value: 0.24929577112197876 and parameters: {'hidden_channels': 192, 'num_layers': 4, 'dropout_prob': 0.26028043216946994, 'learning_rate': 0.0007795508872636925, 'weight_decay': 7.183724136797134e-05}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:55,538] Trial 38 finished with value: 0.5045359134674072 and parameters: {'hidden_channels': 64, 'num_layers': 5, 'dropout_prob': 0.25162331975658514, 'learning_rate': 2.0635084838533566e-05, 'weight_decay': 0.0001705319759234457}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:56,832] Trial 39 finished with value: 0.2615983486175537 and parameters: {'hidden_channels': 192, 'num_layers': 4, 'dropout_prob': 0.23548270064748827, 'learning_rate': 0.00024141896569799157, 'weight_decay': 7.020420089482018e-05}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:58,198] Trial 40 finished with value: 0.42951011657714844 and parameters: {'hidden_channels': 256, 'num_layers': 3, 'dropout_prob': 0.46754038101551815, 'learning_rate': 0.00010989231385559995, 'weight_decay': 0.0004467976521467105}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:33:59,843] Trial 41 finished with value: 0.18460865318775177 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.21356141562116204, 'learning_rate': 0.0015107055631196308, 'weight_decay': 0.0012914168327137368}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:34:01,434] Trial 42 finished with value: 0.17428791522979736 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.22946367897499045, 'learning_rate': 0.0008987615012305923, 'weight_decay': 0.0011773948089487355}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:34:03,079] Trial 43 finished with value: 0.1767435073852539 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.2309540453148123, 'learning_rate': 0.0008603168714106466, 'weight_decay': 0.0007432679827991171}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:34:04,488] Trial 44 finished with value: 0.1823699027299881 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.23474598001201138, 'learning_rate': 0.0007122151657039526, 'weight_decay': 0.00010500175674723912}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:34:05,911] Trial 45 finished with value: 0.19453182816505432 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.30826827557694614, 'learning_rate': 0.0007265598545829231, 'weight_decay': 0.0001149586084012698}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:34:07,121] Trial 46 finished with value: 0.3684781491756439 and parameters: {'hidden_channels': 256, 'num_layers': 2, 'dropout_prob': 0.23432928072356923, 'learning_rate': 0.0003054237605798597, 'weight_decay': 0.00023689265922813543}. Best is trial 34 with value: 0.17314834892749786.\n",
      "[I 2025-01-15 16:34:08,808] Trial 47 finished with value: 0.17154620587825775 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.2640480150596121, 'learning_rate': 0.00048407082866720226, 'weight_decay': 4.739591553409838e-05}. Best is trial 47 with value: 0.17154620587825775.\n",
      "[I 2025-01-15 16:34:10,491] Trial 48 finished with value: 0.17200174927711487 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.263859558731383, 'learning_rate': 0.00047299083828018947, 'weight_decay': 2.913994306251578e-05}. Best is trial 47 with value: 0.17154620587825775.\n",
      "[I 2025-01-15 16:34:12,167] Trial 49 finished with value: 0.17086650431156158 and parameters: {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.29322510010713654, 'learning_rate': 0.00047694845293874173, 'weight_decay': 1.807500817650948e-05}. Best is trial 49 with value: 0.17086650431156158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'hidden_channels': 256, 'num_layers': 5, 'dropout_prob': 0.29322510010713654, 'learning_rate': 0.00047694845293874173, 'weight_decay': 1.807500817650948e-05}\n",
      "\n",
      "Starting training...\n",
      "Epoch 10/1000, Train Loss: 0.4862, Val Loss: 0.4335\n",
      "Epoch 20/1000, Train Loss: 0.3338, Val Loss: 0.3449\n",
      "Epoch 30/1000, Train Loss: 0.3743, Val Loss: 0.2774\n",
      "Epoch 40/1000, Train Loss: 0.3263, Val Loss: 0.3012\n",
      "Epoch 50/1000, Train Loss: 0.2323, Val Loss: 0.2254\n",
      "Epoch 60/1000, Train Loss: 0.2097, Val Loss: 0.2141\n",
      "Epoch 70/1000, Train Loss: 0.3080, Val Loss: 0.2086\n",
      "Epoch 80/1000, Train Loss: 0.2222, Val Loss: 0.2110\n",
      "Epoch 90/1000, Train Loss: 0.1938, Val Loss: 0.1909\n",
      "Epoch 100/1000, Train Loss: 0.2805, Val Loss: 0.2117\n",
      "Epoch 110/1000, Train Loss: 0.2021, Val Loss: 0.1913\n",
      "Epoch 120/1000, Train Loss: 0.2560, Val Loss: 0.1813\n",
      "Epoch 130/1000, Train Loss: 0.2017, Val Loss: 0.1720\n",
      "Epoch 140/1000, Train Loss: 0.2074, Val Loss: 0.1697\n",
      "Epoch 150/1000, Train Loss: 0.1944, Val Loss: 0.1721\n",
      "Epoch 160/1000, Train Loss: 0.1249, Val Loss: 0.1666\n",
      "Epoch 170/1000, Train Loss: 0.2110, Val Loss: 0.1695\n",
      "Epoch 180/1000, Train Loss: 0.1596, Val Loss: 0.1653\n",
      "Epoch 190/1000, Train Loss: 0.2053, Val Loss: 0.1587\n",
      "Epoch 200/1000, Train Loss: 0.2075, Val Loss: 0.1663\n",
      "Epoch 210/1000, Train Loss: 0.1912, Val Loss: 0.1567\n",
      "Epoch 220/1000, Train Loss: 0.1830, Val Loss: 0.1584\n",
      "Epoch 230/1000, Train Loss: 0.1857, Val Loss: 0.1597\n",
      "\n",
      "Early stopping triggered at epoch 236\n",
      "\n",
      "Evaluating model...\n",
      "Accuracy: 0.9375\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.87      0.93        15\n",
      "         1.0       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.94        32\n",
      "   macro avg       0.95      0.93      0.94        32\n",
      "weighted avg       0.94      0.94      0.94        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3625620/3807466596.py:211: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# Enhanced GCN Model with improved structure and handling\n",
    "class EnhancedGCNWithEdgeFeatures(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_channels, hidden_channels=128, num_layers=3, dropout_prob=0.5):\n",
    "        super(EnhancedGCNWithEdgeFeatures, self).__init__(aggr='add')\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.node_projection = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.edge_projection = torch.nn.Linear(edge_channels, hidden_channels)\n",
    "\n",
    "        self.node_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.LayerNorm(hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.edge_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.LayerNorm(hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "        self.skip_transforms = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "            for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        self.final_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            torch.nn.LayerNorm(hidden_channels // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_prob),\n",
    "            torch.nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.node_projection(x)\n",
    "        edge_attr = self.edge_projection(edge_attr)\n",
    "        previous_layer = None\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            current_x = self.node_transforms[i](x if i == 0 else previous_layer)\n",
    "            if i > 0:\n",
    "                current_x = current_x + self.skip_transforms[i - 1](previous_layer)\n",
    "            current_x = self.propagate(\n",
    "                edge_index,\n",
    "                x=current_x,\n",
    "                edge_attr=self.edge_transforms[i](edge_attr)\n",
    "            )\n",
    "            previous_layer = current_x\n",
    "\n",
    "        return torch.sigmoid(self.final_layers(current_x))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        attention_input = torch.cat([x_i, x_j], dim=-1)\n",
    "        attention_weights = torch.softmax(self.attention(attention_input), dim=-1)\n",
    "        combined_features = x_j + edge_attr\n",
    "        return attention_weights * combined_features\n",
    "\n",
    "\n",
    "# Objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 64, 256, step=64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "\n",
    "    # Initialize model\n",
    "    model = EnhancedGCNWithEdgeFeatures(\n",
    "        in_channels=node_features.shape[1],\n",
    "        edge_channels=edge_features.shape[1],\n",
    "        hidden_channels=hidden_channels,\n",
    "        num_layers=num_layers,\n",
    "        dropout_prob=dropout_prob\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer and scheduler setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "    # Early stopping setup\n",
    "    patience = 20\n",
    "    best_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(node_features, edge_index, edge_features)\n",
    "        train_loss = F.binary_cross_entropy(out.squeeze(), y_node)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation (simulated here; replace with actual validation data)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(node_features, edge_index, edge_features)  # Replace with validation data\n",
    "            val_loss = F.binary_cross_entropy(val_out.squeeze(), y_node)  # Replace y_node with validation labels\n",
    "\n",
    "        # Scheduler and early stopping\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Return the validation loss for Optuna optimization\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Assuming node_features, edge_index, edge_features, and y_node are already prepared\n",
    "node_features = node_features.to(device)\n",
    "edge_index = graph_data.edge_index.to(device)\n",
    "edge_features = edge_features.to(device)\n",
    "y_node = y_node.to(device)\n",
    "\n",
    "# Optuna hyperparameter tuning\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Initialize model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "model = EnhancedGCNWithEdgeFeatures(\n",
    "    in_channels=node_features.shape[1],\n",
    "    edge_channels=edge_features.shape[1],\n",
    "    hidden_channels=best_params['hidden_channels'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout_prob=best_params['dropout_prob']\n",
    ").to(device)\n",
    "\n",
    "# Final training with best hyperparameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 20\n",
    "best_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(node_features, edge_index, edge_features)\n",
    "    train_loss = F.binary_cross_entropy(out.squeeze(), y_node)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation (simulated here; replace with actual validation data)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(node_features, edge_index, edge_features)  # Replace with validation data\n",
    "        val_loss = F.binary_cross_entropy(val_out.squeeze(), y_node)  # Replace y_node with validation labels\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Scheduler and early stopping\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(node_features, edge_index, edge_features)\n",
    "    pred_labels = (pred.squeeze() > 0.5).float()\n",
    "    accuracy = accuracy_score(y_node.cpu(), pred_labels.cpu())\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_node.cpu(), pred_labels.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now training the model with the percet hyperparametes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnleak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
