{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3990645/2504979202.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load('graph_data_new_topology.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and prepare the graph data\n",
    "print(\"Loading graph data...\")\n",
    "graph_data = torch.load('graph_data_new_topology.pt')\n",
    "print(\"Graph data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data.edge_features = torch.cat([graph_data.edge_features, graph_data.edge_features], dim=0)\n",
    "edges = torch.cat([graph_data.edge_index[0], graph_data.edge_index[1]])\n",
    "edges_reversed = torch.stack([reversed(e) for e in edges.reshape(-1, 2)]).flatten()\n",
    "graph_data.edge_index = torch.stack([edges, edges_reversed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[32, 17520000] i64 n=560640000 (4.2Gb) x∈[0, 1] μ=0.008 σ=0.087"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc1 = F.one_hot(graph_data.y_location_1.long())[:, 1:]\n",
    "loc2 = F.one_hot(graph_data.y_location_2.long())[:, 1:]\n",
    "y = (loc1 + loc2).T\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = Data(\n",
    "    edge_index=graph_data.edge_index,\n",
    "    num_nodes=graph_data.num_nodes,\n",
    "    node_features=graph_data.node_features,\n",
    "    edge_features=graph_data.edge_features,\n",
    "    y=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 68], y=[32, 17520000], num_nodes=32, node_features=[32, 17520000], edge_features=[68, 17520000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(graph_data, \"graph_data_new_topology_preprocessed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3990645/157294050.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_data = torch.load(\"graph_data_new_topology_preprocessed.pt\")\n"
     ]
    }
   ],
   "source": [
    "graph_data = torch.load(\"graph_data_new_topology_preprocessed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated graph data structure:\n",
      "y: tensor[32, 1000000] i64 n=32000000 (0.2Gb) x∈[0, 1] μ=0.009 σ=0.092\n",
      "node_features: tensor[32, 1000000] n=32000000 (0.1Gb) x∈[0., 69.984] μ=61.334 σ=11.887\n",
      "edge_features: tensor[68, 1000000] n=68000000 (0.3Gb) x∈[-2.045e+03, 1.263e+04] μ=841.751 σ=1.522e+03\n",
      "\n",
      "Reshaping data for GCN...\n",
      "Reshaped node features: tensor[32, 99999] n=3199968 (12Mb) x∈[0., 69.967] μ=61.334 σ=11.615\n",
      "Reshaped edge features: tensor[68, 99999] n=6799932 (26Mb) x∈[-1.758e+03, 1.103e+04] μ=841.755 σ=1.479e+03\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clip the graph data to the first million data points\n",
    "num_data_points = 1_000_000  # Define the limit for the data points\n",
    "graph_data.node_features = graph_data.node_features[:, :num_data_points]\n",
    "graph_data.edge_features = graph_data.edge_features[:, :num_data_points]\n",
    "graph_data.y = graph_data.y[:, :num_data_points]\n",
    "\n",
    "# Verify the updated data shape\n",
    "print(\"Updated graph data structure:\")\n",
    "print(f\"y: {graph_data.y}\")\n",
    "print(f\"node_features: {graph_data.node_features}\")\n",
    "print(f\"edge_features: {graph_data.edge_features}\")\n",
    "\n",
    "# Step 3: Reshape node and edge features for GCN\n",
    "def reshape_for_gcn(data, window_size=20, stride=10):\n",
    "    \"\"\"\n",
    "    Reshape time-series data for GCN format. \n",
    "    Returns reshaped data with shape [num_elements, num_windows].\n",
    "    \"\"\"\n",
    "    num_elements, num_timesteps = data.shape\n",
    "    num_windows = (num_timesteps - window_size) // stride + 1\n",
    "    \n",
    "    reshaped_data = torch.zeros((num_elements, num_windows))\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        reshaped_data[:, i] = data[:, start_idx:end_idx].mean(dim=1)\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "# Reshape node and edge features\n",
    "print(\"\\nReshaping data for GCN...\")\n",
    "graph_data.node_features = reshape_for_gcn(graph_data.node_features, window_size=20, stride=10)\n",
    "graph_data.edge_features = reshape_for_gcn(graph_data.edge_features, window_size=20, stride=10)\n",
    "print(f\"Reshaped node features: {graph_data.node_features}\")\n",
    "print(f\"Reshaped edge features: {graph_data.edge_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_labels(data, window_size=20, stride=10):\n",
    "    num_elements, num_timesteps = data.shape\n",
    "    num_windows = (num_timesteps - window_size) // stride + 1\n",
    "    \n",
    "    reshaped_data = torch.zeros((num_elements, num_windows))\n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        reshaped_data[:, i] = data[:, start_idx:end_idx].mean(dim=1) > 0.5\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "    \n",
    "y = aggregate_labels(graph_data.y.float()).long()\n",
    "graph_data.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:10<00:00, 9949.43it/s] \n"
     ]
    }
   ],
   "source": [
    "num_graphs = graph_data.y.shape[1]\n",
    "graphs = []\n",
    "for i in tqdm(range(num_graphs)):\n",
    "    graph = Data(\n",
    "        x=graph_data.node_features[:, i].unsqueeze(1),\n",
    "        edge_index=graph_data.edge_index,\n",
    "        edge_attr=graph_data.edge_features[:, i].unsqueeze(1),\n",
    "        y=graph_data.y[:, i]\n",
    "    )\n",
    "    graphs.append(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EDGE_FEATURES = 1\n",
    "NUM_NODE_FEATURES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically the same as the baseline except we pass edge features \n",
    "class GDPModel(torch.nn.Module):\n",
    "    def __init__(self, num_features=3, hidden_size=32, target_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.target_size = target_size\n",
    "        self.convs = [GATConv(self.num_features, self.hidden_size, edge_dim = NUM_EDGE_FEATURES),\n",
    "                      GATConv(self.hidden_size, self.hidden_size, edge_dim = NUM_EDGE_FEATURES)]\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr) # adding edge features here!\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr=edge_attr) # edge features here as well\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32]),\n",
       " Data(x=[32, 1], edge_index=[2, 68], edge_attr=[68, 1], y=[32])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Graph Data\n",
      "79999\n",
      "Test Graph Data\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.2\n",
    "train_graphs, test_graphs = train_test_split(\n",
    "    graphs, \n",
    "    test_size=TEST_SIZE\n",
    "    )\n",
    "print(\"Train Graph Data\")\n",
    "print(len(train_graphs))\n",
    "print(\"Test Graph Data\")\n",
    "print(len(test_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_data):\n",
    "    loader = DataLoader(val_data, batch_size=len(val_data), shuffle=True)\n",
    "    data = next(iter(loader))\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(data)\n",
    "    y_pred_leak = (y_pred.sum(dim=1) > 0).int()\n",
    "    # y_true_leak = (data.y.sum(dim=1) > 0).int()\n",
    "    print(classification_report(y_pred_leak, data.y))\n",
    "    loss = F.cross_entropy(y_pred, data.y)\n",
    "    return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, name_prefix, hyperparams):\n",
    "    ''' \n",
    "    Train model with given hyperparams dict.\n",
    "    Saves the following CSVs over the course of training:\n",
    "    1. the loss trajectory: the val and train loss every save_loss_interval epochs at\n",
    "       filename 'results/{name_prefix}_{learning_rate}_train.csv' e.g. 'results/baseline_0.05_train.csv'\n",
    "    2. every save_model_interval save both the model at e.g. 'models/baseline_0.05_0_out_of_1000.pt`\n",
    "       and the predicted values vs actual values in `results/baseline_0.05_0_out_of_1000_prediction.csv' on the test data.\n",
    "    '''\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    n_epochs = hyperparams['n_epochs']\n",
    "    save_loss_interval = hyperparams['save_loss_interval']\n",
    "    print_interval = hyperparams['print_interval']\n",
    "    save_model_interval = hyperparams['save_model_interval']\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for data in tqdm(loader):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.cross_entropy(out, data.y)\n",
    "            epoch_loss += loss.item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % save_loss_interval == 0:\n",
    "            val_loss = evaluate_model(model, test_graphs) / len(test_graphs)\n",
    "            train_loss = epoch_loss / len(train_graphs) * batch_size\n",
    "            if epoch % print_interval == 0:\n",
    "                print(\"Epoch: {} Train loss: {:.2e} Validation loss: {:.2e}\".format(epoch, train_loss, val_loss))\n",
    "            losses.append((epoch, train_loss, val_loss))\n",
    "        if epoch % save_model_interval == 0:\n",
    "            # save predictions for plotting\n",
    "            model.eval()\n",
    "        print(loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GDPModel(\n",
       "  (linear): Linear(in_features=64, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GDPModel(num_features=NUM_NODE_FEATURES, hidden_size=64, target_size=graph_data.num_nodes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    learning_rate=0.001,\n",
    "    n_epochs=50,\n",
    "    batch_size=64,\n",
    "    #\n",
    "    save_loss_interval=1,\n",
    "    print_interval=1,\n",
    "    save_model_interval=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:18<00:00, 68.21it/s]\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.01      0.02    640000\n",
      "\n",
      "    accuracy                           0.01    640000\n",
      "   macro avg       0.50      0.00      0.01    640000\n",
      "weighted avg       1.00      0.01      0.02    640000\n",
      "\n",
      "Epoch: 0 Train loss: 4.96e-02 Validation loss: 2.44e-06\n",
      "tensor grad NllLossBackward0 0.045\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:17<00:00, 70.42it/s]\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.01      0.02    640000\n",
      "\n",
      "    accuracy                           0.01    640000\n",
      "   macro avg       0.50      0.00      0.01    640000\n",
      "weighted avg       1.00      0.01      0.02    640000\n",
      "\n",
      "Epoch: 1 Train loss: 4.96e-02 Validation loss: 2.44e-06\n",
      "tensor grad NllLossBackward0 0.047\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:17<00:00, 71.11it/s]\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/pasquale/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, name_prefix, hyperparams)\u001b[0m\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_loss_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_graphs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_graphs)\n\u001b[1;32m     33\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_graphs) \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m print_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[63], line 8\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_data)\u001b[0m\n\u001b[1;32m      6\u001b[0m y_pred_leak \u001b[38;5;241m=\u001b[39m (y_pred\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# y_true_leak = (data.y.sum(dim=1) > 0).int()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_leak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(y_pred, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2742\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2739\u001b[0m     line_heading \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m avg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;66;03m# compute averages with specified averaging method\u001b[39;00m\n\u001b[0;32m-> 2742\u001b[0m avg_p, avg_r, avg_f1, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2750\u001b[0m avg \u001b[38;5;241m=\u001b[39m [avg_p, avg_r, avg_f1, np\u001b[38;5;241m.\u001b[39msum(s)]\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1834\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1834\u001b[0m MCM \u001b[38;5;241m=\u001b[39m \u001b[43mmultilabel_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamplewise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplewise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1841\u001b[0m tp_sum \u001b[38;5;241m=\u001b[39m MCM[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1842\u001b[0m pred_sum \u001b[38;5;241m=\u001b[39m tp_sum \u001b[38;5;241m+\u001b[39m MCM[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:562\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    560\u001b[0m le\u001b[38;5;241m.\u001b[39mfit(labels)\n\u001b[1;32m    561\u001b[0m y_true \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mtransform(y_true)\n\u001b[0;32m--> 562\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m sorted_labels \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# labels are now from 0 to len(labels) - 1 -> use bincount\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:134\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/utils/_encode.py:243\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(diff)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_searchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniques\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/sklearn/utils/_array_api.py:944\u001b[0m, in \u001b[0;36m_searchsorted\u001b[0;34m(a, v, side, sorter, xp)\u001b[0m\n\u001b[1;32m    942\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(a, v, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchsorted\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m a_np \u001b[38;5;241m=\u001b[39m _convert_to_numpy(a, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    947\u001b[0m v_np \u001b[38;5;241m=\u001b[39m _convert_to_numpy(v, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:1527\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;124;03m    30  # The element at index 2 of the sorted array is 30.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnleak/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, \"\", hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnleak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
